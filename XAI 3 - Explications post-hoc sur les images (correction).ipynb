{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "binary-plaintiff",
   "metadata": {
    "id": "binary-plaintiff"
   },
   "source": [
    "<center><img src='https://drive.google.com/uc?export=view&id=1qJ8NqAZolTBQY7lN-deZ8xEsU3dlUiLz' width=200></center>\n",
    "\n",
    "\n",
    "<h6><center></center></h6>\n",
    "\n",
    "<h1>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "<center>Cours XAI</center>\n",
    "    <center> Activités 2 : Explications post-hoc sur images</center>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-paraguay",
   "metadata": {
    "id": "personalized-paraguay"
   },
   "source": [
    "Dans ce notebook, nous allons nous intéresser à des modèles type \"Réseaux de neurones\" et utiliser quelques outils pour les interpréter post-hoc.\n",
    "\n",
    "# Prérequis\n",
    "\n",
    "En plus des modules que nous avons déjà installé, vous aurez besoin des modules suivants: keras, tensorflow.\n",
    "\n",
    "Notez que tensorflow ne fonctionne pas avec python 3.9. Cependant, vous pouvez installer une version non-stable avec tf-nightly.\n",
    "\n",
    "# Chargement d'un modèle pré-entraîné\n",
    "\n",
    "Nous allons utiliser Keras et le modèle [inception v3](http://arxiv.org/abs/1512.00567). Il s'agit d'un CNN spécialisé dans la reconnaissance d'objets. Nous vous donnons également une petite fonction qui permet de mettre au bon format une image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L5Os7PcwPLYw",
   "metadata": {
    "id": "L5Os7PcwPLYw"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QYE9qK2JPb5M",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4411,
     "status": "ok",
     "timestamp": 1681416650741,
     "user": {
      "displayName": "Wassila Ouerdane",
      "userId": "06311256887956564892"
     },
     "user_tz": -120
    },
    "id": "QYE9qK2JPb5M",
    "outputId": "0e00efd5-3c03-4c14-d54a-36f273502112"
   },
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "consolidated-bicycle",
   "metadata": {
    "executionInfo": {
     "elapsed": 4653,
     "status": "ok",
     "timestamp": 1681419073929,
     "user": {
      "displayName": "Wassila Ouerdane",
      "userId": "06311256887956564892"
     },
     "user_tz": -120
    },
    "id": "consolidated-bicycle"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "from keras import preprocessing\n",
    "import keras.applications\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model= tf.keras.applications.inception_v3.InceptionV3()\n",
    "#model = inception_v3.InceptionV3()\n",
    "\n",
    "def preprocess_img_list(liste):\n",
    "    \"\"\"Prend une liste de chaines de caractères représentant les paths des images à charger \n",
    "       et retourne une collection d'objets utilisables par le CNN\"\"\"\n",
    "    out = []\n",
    "    for img_path in liste:\n",
    "       # img = image.load_img(img_path, target_size=(299, 299))\n",
    "        \n",
    "        image = tf.keras.utils.load_img(img_path,target_size=(299, 299))\n",
    "        x = tf.keras.utils.img_to_array(image)\n",
    "        #img = load_img(img_path, target_size=(299, 299))\n",
    "        #x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = tf.keras.applications.inception_v3.preprocess_input(x)\n",
    "        out.append(x)\n",
    "    return np.vstack(out)\n",
    "\n",
    "def show(img):\n",
    "    \"\"\"Affiche une image prétraitée\"\"\"\n",
    "    plt.imshow(img / 2 + 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-oliver",
   "metadata": {
    "id": "regular-oliver"
   },
   "source": [
    "## Prise en main du modèle\n",
    "\n",
    "Affichez la description de `model`en appelant sa méthode `summary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-ozone",
   "metadata": {
    "id": "grave-ozone"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-patrol",
   "metadata": {
    "id": "editorial-patrol"
   },
   "source": [
    "Pré-traitez l'image `image1.jpg` et affichez-la avec les méthodes `preprocess_img_list` et `show` déclarées ci-dessus. \n",
    "Ensuite, utilisez la méthode `predict` de `model` en lui donnant le résultat de `preprocess_img_list` et stockez les prédictions dans une variable (`preds` par exemple).\n",
    "\n",
    "En utlisant `decode_predictions(preds)`, vous obtiendez une énumération des classes pour chacune des images que vous pouvez parcourir avec un `for`. Affichez les prédictions pour cette image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-privacy",
   "metadata": {
    "id": "broken-privacy"
   },
   "outputs": [],
   "source": [
    "images = preprocess_img_list(['image1.jpg'])\n",
    "show(images[0])\n",
    "preds = model.predict(images)\n",
    "\n",
    "for x in tf.keras.applications.inception_v3.decode_predictions(\n",
    "    preds)[0]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-tuning",
   "metadata": {
    "id": "juvenile-tuning"
   },
   "source": [
    "# LIME\n",
    "\n",
    "Dans cette partie, nous allons appliquer LIME à cette image. LIME va nous donner pour chacune des classes les pixels qui ont conduit à cette décision.\n",
    "\n",
    "## Obtention de l'explication\n",
    "Créez un `LimeImageExplainer`.\n",
    "Utilisez sa méthode `explain_instance` pour obtenir l'explication. Cette méthode prend pour arguments:\n",
    "* l'image, qu'il faut convertir en tableau de double avec `images[0].astype('double')`\n",
    "* la méthode `predict` de `model`\n",
    "\n",
    "Il faudra ensuite fournir les paramètres optionnels suivants:\n",
    "* `top_labels`: indique combien de classes il faut considérer\n",
    "* `hide_color`: couleur utilisée pour les pixels qui ne font pas partie de l'explication. Mettez 0 (gris) \n",
    "* `num_samples`: combien d'échantillons vont être générés par LIME. Attention, comme il s'agit d'images, ça va être long..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RJVFvDClZF2Y",
   "metadata": {
    "id": "RJVFvDClZF2Y"
   },
   "outputs": [],
   "source": [
    "!pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-techno",
   "metadata": {
    "id": "incident-techno"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import lime\n",
    "from lime import lime_image\n",
    "\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanation = explainer.explain_instance(images[0].astype('double'), model.predict, top_labels=5, hide_color=0, num_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-jamaica",
   "metadata": {
    "id": "alert-jamaica"
   },
   "source": [
    "## Affichage de l'explication\n",
    "\n",
    "A présent, vous avez l'explication et il faut l'afficher. Pour cela, utilisez la méthode `get_image_and_mask` de `explanation` qui prend en argument obligatoire:\n",
    "* la classe concernée: typiquement, on lui passe `explanation.top_labels[i]` où i est la ième top classe.\n",
    "\n",
    "Puis, en argument optionnel:\n",
    "* `positive_only`: un booléen qui indique si l'on veut afficher uniquement les pixels qui expliquent positivement la classe. En général, on met True\n",
    "* `num_features`: indique le nombre de superpixels qui s'afficheront\n",
    "* `hide_rest`: booléen qui indique que la partie non explicative de l'image soit grisée. Générallement, False.\n",
    "\n",
    "Cette fonction retourne deux résultats: un résultat intermédiaire et un masque.\n",
    "On vous donne une fonction qui affiche l'image résultat et qui prend `temp`et `mask` en paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-bowling",
   "metadata": {
    "id": "widespread-bowling"
   },
   "outputs": [],
   "source": [
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "def show(temps, mask):\n",
    "    plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n",
    "\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)\n",
    "\n",
    "show(temp, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-perspective",
   "metadata": {
    "id": "wrapped-perspective"
   },
   "source": [
    "## Interprétation\n",
    "\n",
    "Testez sur les différentes classes détectées. Interprétez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-physiology",
   "metadata": {
    "id": "sophisticated-physiology"
   },
   "outputs": [],
   "source": [
    "#Interprétez ici dans une string \"\"\" \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-clinton",
   "metadata": {
    "id": "encouraging-clinton"
   },
   "source": [
    "# GradCAM\n",
    "\n",
    "GradCAM est une autre méthode pour connaître l'importance des pixels d'une image dans la prise de décision. Pour rappel, elle utilise la dernière couche convolutionnelle d'un CNN.\n",
    "\n",
    "Nous allons vous guider pas à pas dans l'implémentation de GradCAM. En effet, il n'y a pas d'implémentation bien packagée (ou il y a un conflit de version de scipy entre LIME et GradCAM, etc.). \n",
    "\n",
    "Le tutoriel de base est [ici](https://keras.io/examples/vision/grad_cam/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-check",
   "metadata": {
    "id": "known-check"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Méthodes d'affichage\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# On définit ici des constantes et des raccourcis de méthodes\n",
    "model_builder = keras.applications.xception.Xception\n",
    "img_size = (299, 299)\n",
    "preprocess_input = keras.applications.xception.preprocess_input\n",
    "decode_predictions = keras.applications.xception.decode_predictions\n",
    "\n",
    "# image à traiter\n",
    "img_path = \"image1.jpg\"\n",
    "\n",
    "display(Image(img_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-cutting",
   "metadata": {
    "id": "typical-cutting"
   },
   "source": [
    "Vous devez donner ici le nom exact de la dernière couche de convolution. Utilisez l'inspection du modèle plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "funny-administration",
   "metadata": {
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1681419320046,
     "user": {
      "displayName": "Wassila Ouerdane",
      "userId": "06311256887956564892"
     },
     "user_tz": -120
    },
    "id": "funny-administration"
   },
   "outputs": [],
   "source": [
    "last_conv_layer_name = \"block14_sepconv2_act\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-turkish",
   "metadata": {
    "id": "macro-turkish"
   },
   "source": [
    "On va créer une méthode qui formatte correctement notre image. Pour cela, on va utiliser :\n",
    "- keras.preprocessing.image.load_img pour charger l'image à la bonne taille. On obtient ainsi une image PIL\n",
    "- keras.preprocessing.image.img_to_array qui prend l'image PIL et la transforme en un numpy array de float32 de taille (299, 299, 3)\n",
    "- np.expand_dims pour créer un numpy array de dimension (1, 299, 299, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "silent-hawaii",
   "metadata": {
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1681419369768,
     "user": {
      "displayName": "Wassila Ouerdane",
      "userId": "06311256887956564892"
     },
     "user_tz": -120
    },
    "id": "silent-hawaii"
   },
   "outputs": [],
   "source": [
    "def get_img_array(img_path, size):\n",
    "    \"\"\" \n",
    "        From an image path and the size (299x299 for inception 3), gets an array of one image.\n",
    "    \"\"\"\n",
    "    \n",
    "    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n",
    "    array = keras.preprocessing.image.img_to_array(img)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-michigan",
   "metadata": {
    "id": "aboriginal-michigan"
   },
   "source": [
    "Voici la fonction qui calcule la heatmap. Elle n'est pas simple à écrire, il faut lire l'article entier pour pouvoir l'implémenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cosmetic-police",
   "metadata": {
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1681419373661,
     "user": {
      "displayName": "Wassila Ouerdane",
      "userId": "06311256887956564892"
     },
     "user_tz": -120
    },
    "id": "cosmetic-police"
   },
   "outputs": [],
   "source": [
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # On crée dans un premier temps un modèle qui mappe l'image d'entrée\n",
    "    # et les activations de la dernière couche de convolution\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Puis on calcule le gradient des top classes pour l'image d'entrée\n",
    "    # en fonction des gradients de la dernière couche de convolution\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # Ceci correspond au gradient du neurone de sortie (top classe)\n",
    "    # en fonction de la feature map de sortie de la dernière couche de convolution.\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # Tableau dont chaque cellule est l'intensté moyenne du gradient\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # On multiplie chaque cellule du tableau \n",
    "    # par \"combien cette cellule est importante\" par rapport à la top classe\n",
    "    # et on somme tout pour obtenir la heatmap\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # Pour faciliter la visualisation on normalise la heatmap entre 0 et 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-leone",
   "metadata": {
    "id": "anticipated-leone"
   },
   "source": [
    "A présent, on va exploiter les méthodes définies ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-overall",
   "metadata": {
    "id": "afraid-overall"
   },
   "outputs": [],
   "source": [
    "# Appeler preprocess_input sur notre image avec la bonne taille (constante définie au-dessus)\n",
    "img_array = preprocess_input(get_img_array(img_path, size=img_size))\n",
    "\n",
    "# On récupère le modèle\n",
    "model = model_builder(weights=\"imagenet\")\n",
    "\n",
    "# On supprime le softmax de la dernière couche\n",
    "model.layers[-1].activation = None\n",
    "\n",
    "# Affiche la top classe\n",
    "preds = model.predict(img_array)\n",
    "print(\"Predicted:\", decode_predictions(preds, top=1)[0])\n",
    "\n",
    "# Calcule de la heatmap\n",
    "heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-principle",
   "metadata": {
    "id": "medical-principle"
   },
   "source": [
    "On écrit et on appelle une fonction pour faire un joli affichage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-spotlight",
   "metadata": {
    "id": "moderate-spotlight"
   },
   "outputs": [],
   "source": [
    "def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n",
    "    # Load the original image\n",
    "    img = keras.preprocessing.image.load_img(img_path)\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "    # Rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Create an image with RGB colorized heatmap\n",
    "    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "\n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "\n",
    "    # Save the superimposed image\n",
    "    superimposed_img.save(cam_path)\n",
    "\n",
    "    # Display Grad CAM\n",
    "    display(Image(cam_path))\n",
    "\n",
    "\n",
    "save_and_display_gradcam(img_path, heatmap)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
